#!/usr/bin/env python3
"""
Test OpenAI Integration
"""

import asyncio
import os
import sys
from pathlib import Path
from dotenv import load_dotenv

def test_openai_configuration():
    print("ğŸš€ Testing OpenAI Integration")
    print("=" * 50)
    
    # Load .env from project root
    project_root = Path(__file__).parent.parent
    env_file = project_root / '.env'
    
    print(f"ğŸ“ Loading .env from: {env_file}")
    load_dotenv(env_file)
    
    # Check configuration
    provider = os.getenv('LLM_PROVIDER', 'openai')
    api_key = os.getenv('OPENAI_API_KEY')
    model = os.getenv('LLM_MODEL', 'gpt-3.5-turbo')
    
    print(f"ğŸ”§ Provider: {provider}")
    print(f"ğŸ¤– Model: {model}")
    print(f"ğŸ”‘ API Key: {api_key[:10]}...{api_key[-5:] if api_key else 'NOT_FOUND'}")
    
    if not api_key:
        print("âŒ OpenAI API key not found!")
        return False
    
    # Test OpenAI directly
    try:
        import openai
        client = openai.OpenAI(api_key=api_key)
        print("âœ… OpenAI client initialized")
        
        # Test simple API call
        print("ğŸ§ª Testing API call...")
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful hospital supply assistant."},
                {"role": "user", "content": "Say hello briefly"}
            ],
            max_tokens=50,
            temperature=0.3
        )
        
        result = response.choices[0].message.content
        print(f"âœ… API Test Result: {result}")
        return True
        
    except Exception as e:
        print(f"âŒ OpenAI test failed: {e}")
        return False

async def test_llm_integration():
    print("\nğŸ” Testing LLM Integration Module")
    print("=" * 50)
    
    try:
        sys.path.append(str(Path(__file__).parent))
        from llm_integration import IntelligentSupplyAssistant, LLM_CONFIGURED, OPENAI_CONFIGURED
        
        print(f"ğŸ“Š LLM_CONFIGURED: {LLM_CONFIGURED}")
        print(f"ğŸ“Š OPENAI_CONFIGURED: {OPENAI_CONFIGURED}")
        
        if not LLM_CONFIGURED:
            print("âŒ No LLM configured")
            return False
        
        # Initialize assistant
        assistant = IntelligentSupplyAssistant()
        print("âœ… LLM Assistant initialized")
        
        # Test conversation method (if it exists)
        if hasattr(assistant, 'process_conversation'):
            print("ğŸ§ª Testing conversation...")
            response = await assistant.process_conversation("Hello, how are you?")
            print(f"âœ… Response: {response}")
        else:
            print("â„¹ï¸ process_conversation method not available")
        
        return True
        
    except Exception as e:
        print(f"âŒ LLM Integration test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_comprehensive_agent():
    print("\nğŸ¤– Testing Comprehensive AI Agent")
    print("=" * 50)
    
    try:
        sys.path.append(str(Path(__file__).parent))
        from comprehensive_ai_agent import process_agent_conversation
        
        print("ğŸ§ª Testing agent conversation...")
        response = await process_agent_conversation("Hello", "test_user", "test_session")
        
        print(f"âœ… Agent Response: {response.get('response', 'No response')[:100]}...")
        print(f"ğŸ“Š Response type: {type(response)}")
        
        # Test JSON serialization
        import json
        json_response = json.dumps(response, indent=2)
        print(f"âœ… JSON serialization successful!")
        
        return True
        
    except Exception as e:
        print(f"âŒ Comprehensive agent test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    print("ğŸš€ Hospital Supply Platform - OpenAI Integration Test")
    print("=" * 70)
    
    # Test OpenAI configuration
    openai_ok = test_openai_configuration()
    
    # Test LLM integration
    llm_ok = await test_llm_integration()
    
    # Test comprehensive agent
    agent_ok = await test_comprehensive_agent()
    
    print("\n" + "=" * 70)
    print("ğŸ“‹ FINAL RESULTS:")
    print(f"   OpenAI API: {'âœ… WORKING' if openai_ok else 'âŒ FAILED'}")
    print(f"   LLM Integration: {'âœ… WORKING' if llm_ok else 'âŒ FAILED'}")
    print(f"   Comprehensive Agent: {'âœ… WORKING' if agent_ok else 'âŒ FAILED'}")
    
    if openai_ok and llm_ok and agent_ok:
        print("ğŸ‰ All systems operational with OpenAI!")
        print("ğŸ’¡ Your agent will now use OpenAI instead of Gemini")
    else:
        print("âš ï¸ Some issues detected. Check configuration.")

if __name__ == "__main__":
    asyncio.run(main())
